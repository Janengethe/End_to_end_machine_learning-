{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae303583-04e3-4400-b326-b04b83167980",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 40px; color: red\">Wine Quality Prediction using regression model</span>\n",
    "\n",
    "<span style=\"font-size: 40px; color: orange\">Objective:</span>\n",
    "* Develop a machine learning model that predicts the quality of wine based on its chemical attributes.\n",
    "\n",
    "* Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal.\n",
    "* The goal is to model wine quality based on physicochemical tests\n",
    "(see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86b241-b2fe-4537-b3fb-9216f4d89020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cfba430-1092-42ca-bd98-50784bd8fdc1",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Import Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a38345fd-ce37-448e-8f16-b3222c89dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b6f70-3b5e-42da-b148-772363f99804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fcff5e2-98b9-4797-a6eb-f21a4c1276ee",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Load the Datasets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8853891-a5bb-47c7-bf33-5e22428a5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red wine dataset\n",
    "red_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# white Wine dataset\n",
    "white_df = pd.read_csv(\"winequality-white.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ea721-dd06-4d8e-aad2-4ab4194772d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f88a936-0d69-42a1-bdaf-0b70e47c7e48",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd851cc-be5d-4dbb-98d4-ff508974c2f4",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: blue\">Handling Missing data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e45939-3549-4e61-afd0-0131085737a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "red_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf28445e-3f23-4e75-b406-aee3bd0c19d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "white_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5f2c8-7c0e-4073-84a3-b0f5e2c6b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49a0ef87-6044-42a4-a08b-9c02ea65d5c9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: blue\">Feature Scaling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76482dd6-405f-4d4d-aba2-a52a484cbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# red wine\n",
    "red_df = pd.DataFrame(scaler.fit_transform(red_df), columns=red_df.columns)\n",
    "# white wine\n",
    "white_df = pd.DataFrame(scaler.fit_transform(white_df), columns=white_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99406e6a-adcb-42e6-acea-0c117df2c261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4c3997f-0c5e-426b-bff4-015f38d12460",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Data Splitting</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d4cfa8-5f3d-4516-9600-6a40499f510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528360</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>-0.453218</td>\n",
       "      <td>-0.243707</td>\n",
       "      <td>-0.466193</td>\n",
       "      <td>-0.379133</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>1.288643</td>\n",
       "      <td>-0.579207</td>\n",
       "      <td>-0.960246</td>\n",
       "      <td>-0.787823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.967442</td>\n",
       "      <td>-1.391472</td>\n",
       "      <td>0.043416</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.872638</td>\n",
       "      <td>0.624363</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.719933</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>-0.787823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298547</td>\n",
       "      <td>1.297065</td>\n",
       "      <td>-1.186070</td>\n",
       "      <td>-0.169427</td>\n",
       "      <td>0.096353</td>\n",
       "      <td>-0.083669</td>\n",
       "      <td>0.229047</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>-0.331177</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>-0.584777</td>\n",
       "      <td>-0.787823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0      -0.528360          0.961877    -1.391472       -0.453218  -0.243707   \n",
       "1      -0.298547          1.967442    -1.391472        0.043416   0.223875   \n",
       "2      -0.298547          1.297065    -1.186070       -0.169427   0.096353   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0            -0.466193             -0.379133  0.558274  1.288643  -0.579207   \n",
       "1             0.872638              0.624363  0.028261 -0.719933   0.128950   \n",
       "2            -0.083669              0.229047  0.134264 -0.331177  -0.048089   \n",
       "\n",
       "    alcohol   quality  \n",
       "0 -0.960246 -0.787823  \n",
       "1 -0.584777 -0.787823  \n",
       "2 -0.584777 -0.787823  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096114ff-0f66-4a56-a7b5-647ba12ea4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red wine\n",
    "X_red = red_df.drop([\"quality\"], axis=1)\n",
    "y_red = red_df[\"quality\"]\n",
    "X_trainr, X_testr, y_trainr, y_testr = train_test_split(X_red, y_red, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7ce0dd-a691-404d-976b-a10a192d1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# White wine\n",
    "X_wh = white_df.drop([\"quality\"], axis=1)\n",
    "y_wh = white_df[\"quality\"]\n",
    "X_trainw, X_testw, y_trainw, y_testw = train_test_split(X_wh, y_wh, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec25cc-7e37-450f-a377-fead4db30847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c68728cb-ad72-4813-b7d9-a1ee78270350",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Model Training and Evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba73e41-cf2f-4022-9822-17bfe742279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "   R^2 Score: 0.4032\n",
      "   MSE: 0.5984\n",
      "   RMSE: 0.7736\n",
      "   MAE: 0.6237\n",
      "Ridge Regression:\n",
      "   R^2 Score: 0.4032\n",
      "   MSE: 0.5984\n",
      "   RMSE: 0.7736\n",
      "   MAE: 0.6237\n",
      "Lasso Regression:\n",
      "   R^2 Score: -0.0056\n",
      "   MSE: 1.0083\n",
      "   RMSE: 1.0041\n",
      "   MAE: 0.8488\n",
      "support Vector Regression:\n",
      "   R^2 Score: 0.4584\n",
      "   MSE: 0.5430\n",
      "   RMSE: 0.7369\n",
      "   MAE: 0.5640\n",
      "Decision Tree Regression:\n",
      "   R^2 Score: 0.0532\n",
      "   MSE: 0.9494\n",
      "   RMSE: 0.9743\n",
      "   MAE: 0.5729\n",
      "Random Forest Regression:\n",
      "   R^2 Score: 0.5289\n",
      "   MSE: 0.4724\n",
      "   RMSE: 0.6873\n",
      "   MAE: 0.5328\n"
     ]
    }
   ],
   "source": [
    "# Red wine\n",
    "r_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"support Vector Regression\": SVR(),\n",
    "    \"Decision Tree Regression\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regression\": RandomForestRegressor()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, features, target):\n",
    "    predictions = model.predict(features)\n",
    "    mse = mean_squared_error(target, predictions)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(target, predictions)\n",
    "    r2 = r2_score(target, predictions)\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "for name, model in r_models.items():\n",
    "    model.fit(X_trainr, y_trainr)\n",
    "    mse, rmse, mae, r2 = evaluate_model(model, X_testr, y_testr)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"   R^2 Score: {r2:.4f}\")\n",
    "    print(f\"   MSE: {mse:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "\n",
    "# MSE closer to 0 means perfect prediction\n",
    "# MAE Closer to 0 means perfect prediction of the model\n",
    "# R Squared when score is 1, it indicates that the model's predictions perfectly match the actual values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f101e2a-cb00-42a3-9f52-315d0e48c458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75069dae-2957-49cd-bfdd-22428297a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "   R^2 Score: 0.2653\n",
      "   MSE: 0.7256\n",
      "   RMSE: 0.8518\n",
      "   MAE: 0.6620\n",
      "Ridge Regression:\n",
      "   R^2 Score: 0.2652\n",
      "   MSE: 0.7257\n",
      "   RMSE: 0.8519\n",
      "   MAE: 0.6621\n",
      "Lasso Regression:\n",
      "   R^2 Score: -0.0014\n",
      "   MSE: 0.9890\n",
      "   RMSE: 0.9945\n",
      "   MAE: 0.7622\n",
      "support Vector Regression:\n",
      "   R^2 Score: 0.3900\n",
      "   MSE: 0.6024\n",
      "   RMSE: 0.7761\n",
      "   MAE: 0.5832\n",
      "Decision Tree Regression:\n",
      "   R^2 Score: 0.1396\n",
      "   MSE: 0.8497\n",
      "   RMSE: 0.9218\n",
      "   MAE: 0.5519\n",
      "Random Forest Regression:\n",
      "   R^2 Score: 0.5534\n",
      "   MSE: 0.4411\n",
      "   RMSE: 0.6641\n",
      "   MAE: 0.4698\n"
     ]
    }
   ],
   "source": [
    "# White wine\n",
    "w_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"support Vector Regression\": SVR(),\n",
    "    \"Decision Tree Regression\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regression\": RandomForestRegressor()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, features, target):\n",
    "    predictions = model.predict(features)\n",
    "    mse = mean_squared_error(target, predictions)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(target, predictions)\n",
    "    r2 = r2_score(target, predictions)\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "for name, model in w_models.items():\n",
    "    model.fit(X_trainw, y_trainw)\n",
    "    mse, rmse, mae, r2 = evaluate_model(model, X_testw, y_testw)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"   R^2 Score: {r2:.4f}\")\n",
    "    print(f\"   MSE: {mse:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32951a47-7cf9-4339-a54b-72b3234e38e2",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Model Tuning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa7c251-5ef1-42e7-b770-44af5a0efb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best Model R^2 Score: 0.530580433060865\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "# Grid Search for Red Wine\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_trainr, y_trainr)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "best_rf_model_score = best_rf_model.score(X_testr, y_testr)\n",
    "print(\"Best Model R^2 Score:\", best_rf_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2dec2e-61db-4556-ba33-3d8dd20eb21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bebf25d-92cc-4f44-bbe3-4fc104071e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Best Model R^2 Score: 0.5520738958112622\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "# Randomised Search for White wine\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, cv=5, scoring='neg_mean_squared_error', n_iter=10, random_state=42)\n",
    "random_search.fit(X_trainw, y_trainw)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf_model = random_search.best_estimator_\n",
    "best_rf_model_score = best_rf_model.score(X_testw, y_testw)\n",
    "print(\"Best Model R^2 Score:\", best_rf_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298ce60-0d48-45e8-8827-5f21e4599a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dadb7d4c-dc92-4f5e-9726-d0515c356351",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: blue\">Feature Selection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77ca357-60a3-4d5f-8562-9c4cb6b5be7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_red_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(best_red_model, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trainr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the selected features and their ranks\u001b[39;00m\n\u001b[1;32m      9\u001b[0m red_sfeatures \u001b[38;5;241m=\u001b[39m X_trainr\u001b[38;5;241m.\u001b[39mcolumns[selctor\u001b[38;5;241m.\u001b[39msupport_]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:726\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    723\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    724\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m--> 726\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[1;32m    732\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:727\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    723\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    724\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m    726\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m--> 727\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    731\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[1;32m    732\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:32\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[0;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[1;32m     30\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[1;32m     31\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:317\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Compute step score on the previous selection iteration\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# because 'estimator' must use features\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# that have not been eliminated yet\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_score:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstep_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    318\u001b[0m support_[features[ranks][:threshold]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    319\u001b[0m ranking_[np\u001b[38;5;241m.\u001b[39mlogical_not(support_)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py:35\u001b[0m, in \u001b[0;36m_rfe_single_fit.<locals>.<lambda>\u001b[0;34m(estimator, features)\u001b[0m\n\u001b[1;32m     30\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[1;32m     31\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m     33\u001b[0m     X_train,\n\u001b[1;32m     34\u001b[0m     y_train,\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m estimator, features: \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:813\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    811\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_scorer.py:266\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     _kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_scorer.py:355\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[0;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m method_caller(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[1;32m    354\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscoring_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "# Instantiate the best model for Red Wine\n",
    "\n",
    "best_red_model = RandomForestRegressor(n_estimators=300, min_samples_split=2, min_samples_leaf=1, max_depth=None)\n",
    "\n",
    "selector = RFECV(best_red_model, step=1, cv=5, scoring='accuracy')\n",
    "selector.fit(X_trainr, y_trainr)\n",
    "\n",
    "# Get the selected features and their ranks\n",
    "red_sfeatures = X_trainr.columns[selctor.support_]\n",
    "feature_ranks = selector.ranking_\n",
    "\n",
    "print(f\"Selected features: {red_sfeatures}\")\n",
    "print(f\"Feature ranks: {feature_ranks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d81039-f31e-43b7-a070-987b43fcc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the best model from Step 4 (e.g., Random Forests)\n",
    "best_model = RandomForestClassifier(n_estimators=200, max_depth=30, min_samples_split=10, min_samples_leaf=2)\n",
    "\n",
    "# Create the RFECV object and fit it to the training data\n",
    "selector = RFECV(best_model, step=1, cv=5, scoring='accuracy')\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features and their ranks\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "feature_ranks = selector.ranking_\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "print(f\"Feature ranks: {feature_ranks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76fc0b-4c60-4e5f-b69c-3b0b99d2f013",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 30px; color: green\">Saving the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c690d-adb9-462d-9213-ea84cf4fd10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
